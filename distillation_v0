import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
from torchvision.datasets import ImageFolder

from anomalib.models import Patchcore

class StudentModel(nn.Module):
    def __init__(self, output_dim):
        super().__init__()
        self.model = models.resnet18(pretrained=True)
        num_ftrs = self.model.fc.in_features
        self.model.fc = nn.Identity()  # Remove the final FC layer
        self.fc = nn.Linear(num_ftrs, output_dim)  # Add a new FC layer to match Patchcore's output dimension

    def forward(self, x):
        features = self.model(x)
        return self.fc(features)

class DistillationModel(pl.LightningModule):
    def __init__(self, teacher_model, student_model):
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        self.criterion_mse = nn.MSELoss()

    def forward(self, x):
        return self.student(x)

    def training_step(self, batch, batch_idx):
        images, _ = batch

        # Teacher predictions
        with torch.no_grad():
            teacher_features = self.teacher(images)

        # Student predictions
        student_features = self.student(images)

        # Feature matching loss
        loss = self.criterion_mse(student_features, teacher_features)

        self.log('train_loss', loss)
        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.student.parameters(), lr=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        return [optimizer], [scheduler]

def main():
    # Initialize teacher model (Patchcore)
    teacher_model = Patchcore(
        backbone="wide_resnet50_2",
        layers=["layer2", "layer3"],
        pre_trained=True,
        coreset_sampling_ratio=0.1,
        num_neighbors=9,
    )
    teacher_model.eval()

    # Determine the output dimension of the teacher model
    with torch.no_grad():
        dummy_input = torch.randn(1, 3, 224, 224)
        #teacher_output = teacher_model(dummy_input)
        # output_dim = teacher_output.shape[1]
        output_dim = 1536

    # Initialize student model
    student_model = StudentModel(output_dim)

    # Data preparation
    transform = Compose([
        Resize((224, 224)),
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_dataset = ImageFolder(root='/content/drive/MyDrive/patch_core/Project_Phase_2/ITD/type1cam1', transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)

    # Create the distillation model
    distillation_model = DistillationModel(teacher_model, student_model)

    # Training
    trainer = pl.Trainer(max_epochs=100, accelerator='gpu', devices=1)
    trainer.fit(distillation_model, train_loader)

    # Save the distilled student model
    torch.save(student_model.state_dict(), 'distilled_resnet18.pth')

if __name__ == "__main__":
    main()
